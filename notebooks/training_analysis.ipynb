{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Analysis - Italic Detection Model\n",
    "\n",
    "This notebook analyzes the training process and visualizes model performance metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.append('..')\n",
    "import src.config as config\n",
    "\n",
    "# Set plot style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print(\"✅ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Training Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training metrics\n",
    "metrics_path = config.MODEL_DIR / 'indobert-italic' / 'training_metrics.json'\n",
    "\n",
    "if metrics_path.exists():\n",
    "    with open(metrics_path, 'r') as f:\n",
    "        training_metrics = json.load(f)\n",
    "    print(\"✅ Training metrics loaded\")\n",
    "    print(f\"Available metrics: {list(training_metrics.keys())}\")\n",
    "else:\n",
    "    print(\"❌ Training metrics not found. Run training first.\")\n",
    "    print(f\"Expected path: {metrics_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display final metrics\n",
    "if 'training_metrics' in locals():\n",
    "    print(\"Final Validation Metrics:\")\n",
    "    print(\"=\"*50)\n",
    "    for key, value in training_metrics.items():\n",
    "        if isinstance(value, float):\n",
    "            print(f\"{key:20s}: {value:.4f}\")\n",
    "        else:\n",
    "            print(f\"{key:20s}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load TensorBoard Logs\n",
    "\n",
    "Note: This requires TensorBoard event files in the logs directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to load TensorBoard logs\n",
    "try:\n",
    "    from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
    "    \n",
    "    log_dir = config.LOG_DIR\n",
    "    \n",
    "    # Find event files\n",
    "    event_files = list(log_dir.rglob('events.out.tfevents.*'))\n",
    "    \n",
    "    if event_files:\n",
    "        print(f\"✅ Found {len(event_files)} TensorBoard event file(s)\")\n",
    "        \n",
    "        # Load the most recent one\n",
    "        latest_event = sorted(event_files)[-1]\n",
    "        print(f\"Loading: {latest_event}\")\n",
    "        \n",
    "        event_acc = EventAccumulator(str(latest_event.parent))\n",
    "        event_acc.Reload()\n",
    "        \n",
    "        print(f\"Available tags: {event_acc.Tags()}\")\n",
    "    else:\n",
    "        print(\"⚠️  No TensorBoard event files found\")\n",
    "        event_acc = None\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"⚠️  TensorBoard not installed. Install with: pip install tensorboard\")\n",
    "    event_acc = None\n",
    "except Exception as e:\n",
    "    print(f\"⚠️  Error loading TensorBoard logs: {e}\")\n",
    "    event_acc = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if event_acc is not None:\n",
    "    # Extract training loss\n",
    "    try:\n",
    "        train_loss = event_acc.Scalars('train/loss')\n",
    "        steps_train = [x.step for x in train_loss]\n",
    "        values_train = [x.value for x in train_loss]\n",
    "        \n",
    "        # Plot training loss\n",
    "        plt.figure(figsize=(14, 6))\n",
    "        plt.plot(steps_train, values_train, label='Training Loss', color='blue', alpha=0.7)\n",
    "        plt.xlabel('Steps')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training Loss Curve', fontsize=16, fontweight='bold')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    except:\n",
    "        print(\"⚠️  Training loss curve not available\")\n",
    "else:\n",
    "    print(\"⚠️  Cannot plot training curves without TensorBoard logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if event_acc is not None:\n",
    "    # Extract validation metrics\n",
    "    try:\n",
    "        eval_metrics = {}\n",
    "        for metric in ['eval/loss', 'eval/accuracy', 'eval/precision', 'eval/recall', 'eval/f1']:\n",
    "            try:\n",
    "                data = event_acc.Scalars(metric)\n",
    "                eval_metrics[metric] = {\n",
    "                    'steps': [x.step for x in data],\n",
    "                    'values': [x.value for x in data]\n",
    "                }\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        if eval_metrics:\n",
    "            # Plot all metrics\n",
    "            fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "            axes = axes.flatten()\n",
    "            \n",
    "            metric_names = ['eval/accuracy', 'eval/precision', 'eval/recall', 'eval/f1']\n",
    "            titles = ['Validation Accuracy', 'Validation Precision', 'Validation Recall', 'Validation F1 Score']\n",
    "            colors = ['green', 'orange', 'purple', 'red']\n",
    "            \n",
    "            for i, (metric, title, color) in enumerate(zip(metric_names, titles, colors)):\n",
    "                if metric in eval_metrics:\n",
    "                    axes[i].plot(eval_metrics[metric]['steps'], \n",
    "                               eval_metrics[metric]['values'], \n",
    "                               color=color, linewidth=2, marker='o')\n",
    "                    axes[i].set_xlabel('Steps')\n",
    "                    axes[i].set_ylabel(title.split()[1])\n",
    "                    axes[i].set_title(title, fontweight='bold')\n",
    "                    axes[i].grid(True, alpha=0.3)\n",
    "                    \n",
    "                    # Add final value annotation\n",
    "                    final_val = eval_metrics[metric]['values'][-1]\n",
    "                    axes[i].annotate(f'Final: {final_val:.4f}',\n",
    "                                   xy=(eval_metrics[metric]['steps'][-1], final_val),\n",
    "                                   xytext=(10, 10), textcoords='offset points',\n",
    "                                   bbox=dict(boxstyle='round', facecolor='white', alpha=0.8),\n",
    "                                   fontweight='bold')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\"⚠️  No evaluation metrics found in TensorBoard logs\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  Error plotting validation metrics: {e}\")\n",
    "else:\n",
    "    print(\"⚠️  Cannot plot validation curves without TensorBoard logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Test Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test results\n",
    "test_results_path = config.MODEL_DIR / 'indobert-italic' / 'test_results.json'\n",
    "\n",
    "if test_results_path.exists():\n",
    "    with open(test_results_path, 'r') as f:\n",
    "        test_results = json.load(f)\n",
    "    print(\"✅ Test results loaded\")\n",
    "    \n",
    "    # Display classification report\n",
    "    if 'classification_report' in test_results:\n",
    "        print(\"\\nTest Set Classification Report:\")\n",
    "        print(\"=\"*80)\n",
    "        print(test_results['classification_report'])\n",
    "else:\n",
    "    print(\"❌ Test results not found. Run evaluation first.\")\n",
    "    test_results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_results is not None and 'confusion_matrix' in test_results:\n",
    "    cm = np.array(test_results['confusion_matrix'])\n",
    "    labels = ['O', 'B', 'I']\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=labels, yticklabels=labels,\n",
    "                cbar_kws={'label': 'Count'})\n",
    "    plt.xlabel('Predicted Label', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('True Label', fontsize=14, fontweight='bold')\n",
    "    plt.title('Confusion Matrix - Test Set', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate and display per-class accuracy\n",
    "    print(\"\\nPer-Class Accuracy:\")\n",
    "    print(\"=\"*50)\n",
    "    for i, label in enumerate(labels):\n",
    "        accuracy = cm[i, i] / cm[i].sum()\n",
    "        print(f\"{label:5s}: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "else:\n",
    "    print(\"⚠️  Confusion matrix not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary dataframe\n",
    "if 'training_metrics' in locals():\n",
    "    summary_data = {\n",
    "        'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score'],\n",
    "        'Validation': [\n",
    "            training_metrics.get('eval_accuracy', 0),\n",
    "            training_metrics.get('eval_precision', 0),\n",
    "            training_metrics.get('eval_recall', 0),\n",
    "            training_metrics.get('eval_f1', 0)\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    df_summary = pd.DataFrame(summary_data)\n",
    "    \n",
    "    print(\"\\nModel Performance Summary:\")\n",
    "    print(\"=\"*50)\n",
    "    print(df_summary.to_string(index=False))\n",
    "    \n",
    "    # Visualize\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    x = range(len(df_summary))\n",
    "    plt.bar(x, df_summary['Validation'], color='steelblue', alpha=0.7, edgecolor='black')\n",
    "    plt.xticks(x, df_summary['Metric'])\n",
    "    plt.ylabel('Score', fontsize=12)\n",
    "    plt.title('Model Performance Metrics (Validation Set)', fontsize=16, fontweight='bold')\n",
    "    plt.ylim(0, 1.0)\n",
    "    plt.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, v in enumerate(df_summary['Validation']):\n",
    "        plt.text(i, v + 0.02, f'{v:.4f}', ha='center', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory for plots\n",
    "output_dir = config.MODEL_DIR / 'training_visualizations'\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Visualizations can be saved to: {output_dir}\")\n",
    "print(\"\\nTo save plots, add plt.savefig() before plt.show() in cells above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendations\n",
    "\n",
    "Based on the training analysis:\n",
    "\n",
    "1. **Model Convergence**: Check if the training loss has plateaued\n",
    "2. **Overfitting**: Compare training vs validation metrics\n",
    "3. **Class Imbalance**: Review confusion matrix for class-specific issues\n",
    "4. **Early Stopping**: Verify if early stopping triggered appropriately\n",
    "\n",
    "For production deployment:\n",
    "- Ensure F1 score > 0.95 for reliable predictions\n",
    "- Monitor false positive/negative rates\n",
    "- Consider ensemble methods if single model performance insufficient"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
